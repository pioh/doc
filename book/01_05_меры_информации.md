[← К оглавлению](README.md)

---

# [Глава 1.5](01_05_меры_информации.md): Меры информации

## Введение

Как измерить информацию? Сколько информации в книге? В картинке? В сообщении "завтра экзамен"? Оказывается, на этот вопрос нет одного ответа. Информацию можно мерить с трёх разных точек зрения:

1. **Синтаксическая мера** - сколько байтов? (количество, объём)
2. **Семантическая мера** - что это значит? (смысл, новизна)
3. **Прагматическая мера** - насколько это полезно? (ценность, польза)

Одно и то же сообщение может быть огромным по объёму, но бесполезным по сути. Или маленьким, но невероятно ценным. Сейчас разберёмся, как это работает.

## Синтаксическая мера - просто считаем биты

**Синтаксическая (объёмная) мера** - это когда мы просто считаем, сколько битов/байтов занимает информация. Не важно, что там написано - важно, сколько места жрёт.

### Единицы измерения

Основная единица - **бит** (binary digit). Это выбор из двух вариантов: да/нет, 0/1, орёл/решка.

**Производные единицы:**

| Единица    | Символ | Сколько байт           | Сколько это        |
|------------|--------|------------------------|--------------------|
| Бит        | бит    | —                      | 1/8 байта          |
| Байт       | Б (B)  | 1                      | 8 бит              |
| Килобайт   | КБ (KB)| 1 024 (2¹⁰)            | ~1 тыс. байт       |
| Мегабайт   | МБ (MB)| 1 048 576 (2²⁰)        | ~1 млн байт        |
| Гигабайт   | ГБ (GB)| 1 073 741 824 (2³⁰)    | ~1 млрд байт       |
| Терабайт   | ТБ (TB)| 1 099 511 627 776 (2⁴⁰)| ~1 трлн байт       |

**Важно:** В информатике используется бинарная система (степени двойки): 1 КБ = 1024 байта, а не 1000. Производители жёстких дисков часто используют десятичную систему (1000), поэтому диск на "1 ТБ" на самом деле ~931 ГиБ (гибибайт).

### Формула Хартли (для равновероятных событий)

Американец Ральф Хартли в 1928 году придумал формулу:

```
I = log₂(N)
```

Где:
- **I** - количество информации (в битах)
- **N** - количество равновероятных вариантов
- **log₂** - логарифм по основанию 2

**Смысл:** если есть N равновероятных вариантов, то для указания одного из них нужно log₂(N) бит.

**Другая форма:**
```
N = 2^I
```

То есть I битов дают 2^I вариантов.

**Важное следствие:** чтобы удвоить количество вариантов, достаточно добавить всего 1 бит.

### Примеры по формуле Хартли

**Пример 1: Бросание монеты**

2 варианта (орёл/решка):
```
I = log₂(2) = 1 бит
```

Один бросок монеты = **1 бит** информации.

**Пример 2: Игральный кубик**

6 граней:
```
I = log₂(6) ≈ 2.585 бита
```

Бросок кубика ≈ **2.6 бита**.

**Пример 3: Угадай число от 1 до 100**

100 вариантов:
```
I = log₂(100) ≈ 6.644 бита
```

Для указания числа от 1 до 100 нужно примерно **6.6 бита** (или 7 бит в двоичной системе, т.к. 2⁷ = 128 > 100).

**Пример 4: Буква русского алфавита**

33 буквы (если все равновероятны):
```
I = log₂(33) ≈ 5.044 бита
```

Одна русская буква ≈ **5 бит**.

Английский алфавит (26 букв):
```
I = log₂(26) ≈ 4.700 бита
```

Одна английская буква ≈ **4.7 бита**.

**Пример 5: Чёрно-белое изображение**

Изображение 100×100 пикселей, каждый пиксель чёрный или белый:
- Количество пикселей: 100 × 100 = 10 000
- Каждый пиксель: 1 бит (2 варианта)
- Всего: 10 000 бит = 1 250 байт ≈ **1.22 КБ**

Если 256 градаций серого (8 бит на пиксель):
```
I_пиксель = log₂(256) = 8 бит
I_общая = 10 000 × 8 = 80 000 бит = 10 000 байт ≈ 9.77 КБ
```

### Формула Шеннона (для неравновероятных событий)

Клод Шеннон в 1948 году создал теорию информации и придумал формулу для **энтропии** (средней информации на символ):

```
H = -∑(pᵢ × log₂(pᵢ))
```

Где:
- **H** - энтропия (среднее количество информации на символ, в битах)
- **pᵢ** - вероятность появления i-го символа
- **∑** - сумма по всем символам

**Смысл энтропии:** показывает среднее количество информации на один символ. Чем более непредсказуем источник, тем выше энтропия.

**Свойства энтропии:**
1. H ≥ 0 (неотрицательна)
2. H максимальна при равновероятных событиях
3. H = 0 когда одно событие гарантировано (полная предсказуемость, нет информации)

### Примеры по формуле Шеннона

**Пример 6: Неравновероятные символы**

Источник выдаёт три символа: A, B, C с вероятностями:
- p(A) = 0.5
- p(B) = 0.25
- p(C) = 0.25

Энтропия:
```
H = -(0.5 × log₂(0.5) + 0.25 × log₂(0.25) + 0.25 × log₂(0.25))
H = -(0.5 × (-1) + 0.25 × (-2) + 0.25 × (-2))
H = -(-0.5 - 0.5 - 0.5) = 1.5 бита
```

Средняя информация на символ - **1.5 бита**. Это меньше, чем для трёх равновероятных символов (log₂(3) ≈ 1.585 бита), потому что символ A более предсказуем.

**Пример 7: Энтропия русского текста**

В русском языке буквы встречаются с разной частотой:
- "О" - ~11%
- "Е" - ~8.5%
- "А" - ~8%
- ...
- "Ф" - ~0.3%
- "Щ" - ~0.06%

Если бы все 33 буквы были равновероятны:
```
H_макс = log₂(33) ≈ 5.044 бита
```

Но с учётом реальных частот:
```
H_реальная ≈ 4.35 бита на символ
```

**Вывод:** русский текст более предсказуем, чем случайная последовательность букв. Это используется алгоритмами сжатия ([глава 1.12](01_12_сжатие_информации.md)) - можно сжать текст примерно на 14%.

**Пример 8: Информация о редком событии**

Лотерея: вероятность выигрыша 1%, проигрыша 99%.

Сообщение "ты выиграл":
```
I_выигрыш = -log₂(0.01) = log₂(100) ≈ 6.644 бита
```

Сообщение "ты проиграл":
```
I_проигрыш = -log₂(0.99) ≈ 0.0145 бита
```

**Вывод:** сообщение о редком событии (выигрыше) несёт **гораздо больше информации**, чем сообщение об ожидаемом событии (проигрыше). Потому что выигрыш - неожиданность.

## Семантическая мера - важен смысл

**Семантическая мера** - это когда важен **смысл**, **новизна** и **понятность** информации. Одно и то же сообщение может содержать разное количество информации для разных людей.

### Ключевые аспекты

1. **Новизна** - информация есть только если это что-то новое
2. **Понятность** - получатель должен понимать сообщение
3. **Контекст** - смысл зависит от знаний получателя
4. **Достоверность** - ложная информация снижает ценность

### Принцип семантической меры

```
Ic = Iтезаурус / Iсообщение
```

Где:
- **Ic** - семантическая мера (содержательность)
- **Iтезаурус** - знания получателя
- **Iсообщение** - информация в сообщении

**Тезаурус** - это совокупность знаний получателя в данной области.

### Зависимость от тезауруса

**Тезаурус слишком мал** (получатель не понимает):
- Сообщение непонятно
- Семантическая ценность ≈ 0
- Пример: научная статья по квантовой физике для школьника

**Тезаурус оптимален** (соответствует уровню):
- Сообщение понятно и содержит новое
- Максимальная семантическая ценность
- Пример: учебник для студента соответствующего курса

**Тезаурус слишком велик** (получатель уже всё знает):
- Сообщение не содержит нового
- Семантическая ценность ≈ 0
- Пример: учебник начальных классов для профессора

### Примеры семантической меры

**Пример 9: Прогноз погоды**

Сообщение: "Завтра будет дождь" (синтаксически ~150 бит).

Семантически:
- **Для человека, планирующего пикник** - высокая ценность (влияет на планы)
- **Для офисного работника** - средняя ценность
- **Для человека, уже знающего прогноз** - нулевая ценность (нет новизны)

**Пример 10: Теорема Пифагора**

Фраза: "Квадрат гипотенузы равен сумме квадратов катетов"

- **Для ученика 5 класса** - непонятна (недостаточный тезаурус) → семантическая ценность ≈ 0
- **Для ученика 8 класса, изучающего теорему** - новая и понятная → максимальная ценность
- **Для студента матфака** - давно известно → семантическая ценность ≈ 0

**Пример 11: Дублирование информации**

Первое упоминание: "Столица Франции - Париж"
- Для не знающего - высокая семантическая ценность

Второе упоминание той же информации:
- Семантическая ценность = 0 (нет новизны)

Синтаксически оба сообщения одинаковы, семантически второе не несёт информации.

## Прагматическая мера - важна польза

**Прагматическая мера** - это когда важна **полезность**, **ценность** и **влияние на достижение цели** получателя. Не просто смысл, а практическая значимость.

### Ключевые аспекты

1. **Целесообразность** - насколько помогает достичь цели
2. **Своевременность** - информация ценна в нужный момент
3. **Полнота** - достаточность для принятия решения
4. **Достоверность** - надёжность
5. **Актуальность** - соответствие текущей ситуации

### Формализация

```
Ip = P(после) - P(до)
```

Где:
- **Ip** - прагматическая мера
- **P(после)** - вероятность достижения цели после получения информации
- **P(до)** - вероятность до получения информации

Чем больше информация увеличивает вероятность достижения цели, тем она ценнее.

### Примеры прагматической меры

**Пример 12: ДТП на дороге**

Сообщение: "На шоссе М1 произошло ДТП, образовалась пробка"

Синтаксически: ~400 бит  
Семантически: понятно всем водителям

Прагматически:
- **Для водителя, едущего по М1** - очень высокая ценность (можно объехать, сэкономить часы)
- **Для водителя в другом городе** - нулевая ценность
- **Для пешехода** - нулевая ценность

**Пример 13: Биржевая информация**

Информация: "Акции компании X выросли на 15%"

- **Для инвестора, владеющего акциями X** - очень высокая ценность (решение о продаже)
- **Для инвестора с другим портфелем** - низкая ценность
- **Для человека, не занимающегося инвестициями** - нулевая ценность

**Пример 14: Срочность информации**

Информация: "Завтра экзамен перенесён на следующую неделю"

- **Получена за день до экзамена** - очень высокая ценность (можно лучше подготовиться)
- **Получена через неделю после экзамена** - нулевая ценность (устарела)

**Пример 15: Принятие бизнес-решения**

Предприниматель выбирает между проектами A и B.

Информация: "Рынок для проекта A сокращается на 20% ежегодно"

Прагматическая ценность:
- Очень высокая: влияет на выбор, может предотвратить убытки
- Без информации: вероятность успеха 50% (случайный выбор)
- С информацией: вероятность успеха 80% (осознанный выбор проекта B)

```
Ip = 0.8 - 0.5 = 0.3 (или 30%)
```

## Сравнение трёх мер

### Таблица сравнения

| Аспект               | Синтаксическая       | Семантическая        | Прагматическая       |
|----------------------|----------------------|----------------------|----------------------|
| **Что измеряет**     | Объём (биты/байты)   | Смысл, новизна       | Полезность, ценность |
| **Зависит от**       | Размер данных        | Знания получателя    | Цели получателя      |
| **Одинакова для всех**| Да                  | Нет                  | Нет                  |
| **Применение**       | Хранение, передача   | Обучение, поиск      | Принятие решений     |
| **Измеримость**      | Легко (биты, байты)  | Сложно (субъективно) | Очень сложно         |

### Пример на одном сообщении

**Сообщение:** "Температура процессора 95°C"

**Синтаксическая мера:**
- 25 символов × 8 бит = **200 бит = 25 байт**
- Одинакова для всех

**Семантическая мера:**
- Для программиста: **высокая** (понимает значение и последствия)
- Для ребёнка: **низкая** (не понимает контекста)
- Для инженера, уже знающего температуру: **нулевая** (нет новизны)

**Прагматическая мера:**
- Для владельца этого компьютера: **очень высокая** (перегрев, нужно срочно действовать!)
- Для человека без компьютера: **нулевая**
- Для инженера другого компьютера: **нулевая**

### Взаимосвязь

Три меры дополняют друг друга:

```
Синтаксическая → Семантическая → Прагматическая
   (основа)         (понимание)       (применение)
```

1. **Синтаксическая** - база: без физического носителя информации нет ни смысла, ни пользы
2. **Семантическая** - добавляет понимание: бессмысленные данные не могут быть полезны
3. **Прагматическая** - венчает пирамиду: даже понятная информация бесполезна, если неприменима к целям

## Практическое применение

### Синтаксическая мера (технические системы)

- Расчёт ёмкости носителей (HDD, SSD, RAM)
- Оценка скорости передачи данных (Мбит/с)
- Расчёт пропускной способности каналов
- Оптимизация алгоритмов сжатия

Формулы:
```
Время передачи = Размер данных / Скорость канала
Объём хранилища = Количество файлов × Средний размер
```

### Семантическая мера (системы обработки информации)

- Поисковые системы (релевантность документов)
- Системы рекомендаций (персонализация контента)
- Образовательные системы (адаптация материала под уровень)
- Фильтрация спама

### Прагматическая мера (системы поддержки решений)

- Бизнес-аналитика (BI-системы)
- Медицинские диагностические системы
- Финансовые аналитические системы
- Системы управления производством

## Основные термины

**Бит** - минимальная единица информации, выбор из двух равновероятных вариантов.

**Байт** - 8 бит.

**Формула Хартли** - `I = log₂(N)` для расчёта информации при равновероятных событиях.

**Формула Шеннона** - `H = -∑(pᵢ × log₂(pᵢ))` для расчёта энтропии при неравновероятных событиях.

**Энтропия** - мера неопределённости источника, среднее количество информации на символ.

**Синтаксическая мера** - количественная мера (биты/байты), не учитывает смысл.

**Семантическая мера** - учитывает смысл, новизну, понятность для получателя.

**Прагматическая мера** - учитывает полезность и ценность для достижения целей.

**Тезаурус** - совокупность знаний получателя в предметной области.

## Контрольные вопросы

1. **Теория:** Объясни разницу между тремя мерами информации. Приведи пример одного сообщения с разной ценностью по каждой мере.

2. **Хартли:** В базе данных хранятся записи о студентах: номер группы (32 варианта), фамилия (справочник 1024 фамилии), оценка (5 вариантов). Посчитай информацию в каждом поле и общую.

3. **Шеннон:** Источник генерирует четыре символа: A (0.4), B (0.3), C (0.2), D (0.1). Посчитай энтропию. Сравни с энтропией для четырёх равновероятных символов.

4. **Анализ:** Ты получил письмо о конференции по IT. Проанализируй прагматическую ценность для: а) студента-программиста, б) преподавателя информатики, в) пенсионера не из IT.

5. **Применение:** Объясни, почему знание о мерах информации важно при разработке: а) системы резервного копирования, б) образовательной онлайн-платформы, в) системы бизнес-аналитики.

## Связь с другими главами

- [**Глава 1.10**](01_10_свойства_информации.md) - свойства информации
- [**Глава 1.12**](01_12_сжатие_информации.md) - сжатие информации (используется энтропия)

## Резюме

Что запомнить:

✅ **Синтаксическая мера** - считаем биты/байты, не важен смысл  
✅ **Семантическая мера** - важен смысл, новизна, понятность для получателя  
✅ **Прагматическая мера** - важна польза, ценность для достижения целей  
✅ **Формула Хартли**: I = log₂(N) для равновероятных событий  
✅ **Формула Шеннона**: H = -∑(pᵢ × log₂(pᵢ)) для неравновероятных (энтропия)  
✅ Одно и то же сообщение может иметь разную ценность для разных людей  
✅ Три меры дополняют друг друга: объём → смысл → польза  

Теперь ты понимаешь, что информацию можно измерять по-разному. Синтаксически "Привет" и набор случайных символов одинаковы (оба по 6 байт), но семантически и прагматически - небо и земля.

---

_Конец главы_


---

[← К оглавлению](README.md)

*Глава 1.5: Меры информации*
